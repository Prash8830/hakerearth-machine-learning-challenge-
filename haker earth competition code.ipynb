{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fcd661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nadhir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet,SGDRegressor\n",
    "from sklearn.svm import LinearSVR,SVR\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textract\n",
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import wordninja\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer,LancasterStemmer\n",
    "from pyresparser import ResumeParser\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b42969",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_files = test_data[\"CandidateID\"]\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_files = train_data[\"CandidateID\"]\n",
    "train_score = train_data[\"Match Percentage\"]\n",
    "names = []\n",
    "#for name in files:\n",
    "    #names.append(name)q = s_l.reverse()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98b331a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_files_train(train_files,train_score):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    \n",
    "    converted_list = []\n",
    "    cv = CountVectorizer()\n",
    "    mat = []\n",
    "    pure_resumes = []\n",
    "    pure_id = []\n",
    "    pure_score = []\n",
    "    for file,score in zip(train_files,train_score):\n",
    "        path = f'trainResumes/{file}.pdf'\n",
    "        #path = path.replace(\"(\",\"\")\n",
    "        #path = path.replace(\")\",\"\")\n",
    "        #path = path.replace(\",\",\"\")\n",
    "        #path = path.replace(\"'\",\"\")\n",
    "        text = textract.process(path,method='pdfminer')\n",
    "        text = text.decode()\n",
    "\n",
    "        text = text.rstrip('\\r\\n')\n",
    "        \n",
    "        \n",
    "        for element in text:\n",
    "            converted_list.append(element.strip())\n",
    "\n",
    "        e = \"\".join(converted_list)\n",
    "        #print(e)\n",
    "\n",
    "\n",
    "        a = wordninja.split(e)\n",
    "        a = \" \".join(a)\n",
    "        e = a.lower()\n",
    "        e = \" \".join([word for word in e.split() if word not in stop_words])\n",
    "        text_encode = e.encode(encoding=\"ascii\", errors=\"ignore\")\n",
    "        text_decode = text_encode.decode()\n",
    "        clean_text = \" \".join([word for word in text_decode.split()])\n",
    "        \n",
    "        text = re.sub(\"@\\S+\", \"\", clean_text)\n",
    "        text = re.sub(\"\\$\", \"\", text)\n",
    "        text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", text)\n",
    "        text = re.sub(\"#\", \"\", text)\n",
    "        punct = set(string.punctuation) \n",
    "        text = \"\".join([ch for ch in text if ch not in punct])\n",
    "        \n",
    "        words = text.split(\" \")\n",
    "        texts = []\n",
    "        texts.clear()\n",
    "        for word in words: \n",
    "            w = stemmer.stem(word)\n",
    "            texts.append(w)\n",
    "        \n",
    "        text = \" \".join(texts)\n",
    "        \n",
    "        \n",
    "        pure_resumes.append(text)\n",
    "        pure_id.append(file)\n",
    "        pure_score.append(score)\n",
    "\n",
    "        converted_list.clear()    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return pure_id,pure_resumes,pure_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_pdf_files_test(test_files):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    converted_list = []\n",
    "    cv = CountVectorizer()\n",
    "    mat = []\n",
    "    pure_resumes = []\n",
    "    pure_id = []\n",
    "    \n",
    "\n",
    "    for file in zip(test_files):\n",
    "        path = f'testResumes/{file}.pdf'\n",
    "        path = path.replace(\"(\",\"\")\n",
    "        path = path.replace(\")\",\"\")\n",
    "        path = path.replace(\",\",\"\")\n",
    "        path = path.replace(\"'\",\"\")\n",
    "        text = textract.process(f'{path}',method='pdfminer')\n",
    "        text = text.decode()\n",
    "\n",
    "        text = text.rstrip('\\r\\n')\n",
    "\n",
    "        \n",
    "        for element in text:\n",
    "            converted_list.append(element.strip())\n",
    "\n",
    "        e = \"\".join(converted_list)\n",
    "        #print(e)\n",
    "\n",
    "\n",
    "        a = wordninja.split(e)\n",
    "        a = \" \".join(a)\n",
    "        e = a.lower()\n",
    "        e = \" \".join([word for word in e.split() if word not in stop_words])\n",
    "        \n",
    "        text_encode = e.encode(encoding=\"ascii\", errors=\"ignore\")\n",
    "        text_decode = text_encode.decode()\n",
    "        clean_text = \" \".join([word for word in text_decode.split()])\n",
    "        text = re.sub(\"@\\S+\", \"\", clean_text)\n",
    "        text = re.sub(\"\\$\", \"\", text)\n",
    "        text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", text)\n",
    "        text = re.sub(\"#\", \"\", text)\n",
    "        punct = set(string.punctuation) \n",
    "        text = \"\".join([ch for ch in text if ch not in punct])\n",
    "        \n",
    "        words = text.split(\" \")\n",
    "        texts = []\n",
    "        texts.clear()\n",
    "        for word in words: \n",
    "            w = stemmer.stem(word)\n",
    "            texts.append(w)\n",
    "        text = \" \".join(texts)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pure_resumes.append(text)\n",
    "        pure_id.append(file)\n",
    "        converted_list.clear()    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return pure_id,pure_resumes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa8a4f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids,train_resumes,train_scores = read_pdf_files_train(train_files,train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ed616a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids,test_resumes = read_pdf_files_test(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "077c0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(train_resumes)\n",
    "\n",
    "y = vectorizer.transform(test_resumes)\n",
    "features = vectorizer.get_feature_names()\n",
    "x_train= x.toarray()\n",
    "\n",
    "y_test = y.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "da531bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataframe = pd.DataFrame(x_train)\n",
    "training_dataframe.columns = features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "46d828b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 1799)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data_frame = pd.DataFrame(y_test)\n",
    "testing_data_frame.columns = features\n",
    "testing_data_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "82b9de7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['alway', 'apr', 'attent', 'award', 'busi', 'civil', 'cloud', 'comput',\n",
      "       'creat', 'current', 'data', 'de', 'decis', 'driven', 'ece', 'elast',\n",
      "       'explor', 'fresher', 'hand', 'improv', 'junior', 'learn', 'look',\n",
      "       'model', 'network', 'nlp', 'part', 'pattern', 'problem', 'process',\n",
      "       'set', 'solut', 'statist', 'technolog', 'time', 'understand', 'vision'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "lab_enc = preprocessing.LabelEncoder()\n",
    "encoded = lab_enc.fit_transform(train_score)\n",
    "\n",
    "sel = SelectFromModel(ElasticNet(alpha=1.2,random_state=10))\n",
    "sel.fit(training_dataframe, encoded)\n",
    "\n",
    "sel.get_support()\n",
    "selected_feat= training_dataframe.columns[(sel.get_support())]\n",
    "\n",
    "training_d = training_dataframe[selected_feat]\n",
    "\n",
    "\n",
    "\n",
    "sel1= SelectFromModel(SGDRegressor())\n",
    "sel1.fit(training_d, encoded)\n",
    "\n",
    "sel1.get_support()\n",
    "selected_feat= training_d.columns[(sel1.get_support())]\n",
    "\n",
    "print(selected_feat)\n",
    "\n",
    "#best Lasso(),Elastic net(),SGDregressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eab81c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(selected_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ba29e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i get this words from above code\n",
    "\n",
    "c = ['abil', 'alway', 'apr', 'award', 'busi', 'civil', 'cloud', 'comput',\n",
    "       'creat', 'current', 'data', 'de', 'decis', 'driven', 'ece', 'elast',\n",
    "       'execut', 'explor', 'fresher', 'hand', 'improv', 'junior', 'learn',\n",
    "       'look', 'method', 'model', 'nlp', 'object', 'part', 'pattern',\n",
    "       'problem', 'process', 'python', 'scienc', 'set', 'solut', 'technolog',\n",
    "       'time', 'understand', 'vision']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0084a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataframe = training_dataframe[c]\n",
    "testing_data_frame = testing_data_frame[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6c4da5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor,XGBRFRegressor\n",
    "\n",
    "x = np.array(x_train)\n",
    "\n",
    "\n",
    "# Training of the regression model\n",
    "\n",
    "model1 = SGDRegressor()\n",
    "model1.fit(training_dataframe,train_score)\n",
    "\n",
    "model2 = LinearSVR(epsilon=0.5)\n",
    "model2.fit(training_dataframe,train_score)\n",
    "\n",
    "model3 = ElasticNet()\n",
    "model3.fit(training_dataframe,train_score)\n",
    "#good models==linearSVR,Leanear regession,SGDRegressor()\n",
    "\n",
    "#from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "#print(accuracy_score(y_pred,y_test))\n",
    "#print(classification_report(y_test, y_pred))\n",
    "model1_pre = model1.predict(training_dataframe)\n",
    "model2_pre = model2.predict(training_dataframe)\n",
    "model3_pre = model3.predict(training_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "73ac33ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8200069328951431 0.8116719484329183 0.6758667235909476\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(train_score,model1_pre),r2_score(train_score,model2_pre),r2_score(train_score,model3_pre))\n",
    "#0.8276572398355251  accuracy = 89.951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d061290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = model1.predict(testing_data_frame)\n",
    "prediction2 = model2.predict(testing_data_frame)\n",
    "prediction3 = model3.predict(testing_data_frame)\n",
    "#a = pd.DataFrame(prediction,columns=[\"Match Percentage\"])\n",
    "#final = pd.concat([test_files,a],axis=1)\n",
    "#final = final.to_csv(\"submission.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9e2a4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "x = np.array([pd.DataFrame(list(zip(model1_pre,model1_pre,model3_pre)),columns=[\"prediction1\",\"predicition2\",\"prediction3\"])], dtype=np.float32)  # x data (tensor), shape=(100, 1)\n",
    "y = np.array([pd.DataFrame(train_score)], dtype=np.float32)               # noisy y data (tensor), shape=(100, 1)\n",
    "\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "# torch can only train on Variable, so convert them to Variable\n",
    "# The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors\n",
    "# x, y = Variable(x), Variable(y)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden1,n_hidden2, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden1)   # hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.predict = torch.nn.Linear(n_hidden2, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x)) # activation function for hidden layer\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "41ac5f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden1): Linear(in_features=3, out_features=20, bias=True)\n",
      "  (hidden2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "no epochs : 10 and loss : 91.8754653930664\n",
      "no epochs : 20 and loss : 59.34037399291992\n",
      "no epochs : 30 and loss : 46.796180725097656\n",
      "no epochs : 40 and loss : 47.335166931152344\n",
      "no epochs : 50 and loss : 48.252262115478516\n",
      "no epochs : 60 and loss : 46.70525360107422\n",
      "no epochs : 70 and loss : 45.392784118652344\n",
      "no epochs : 80 and loss : 45.142513275146484\n",
      "no epochs : 90 and loss : 44.84651184082031\n",
      "no epochs : 100 and loss : 44.51392364501953\n",
      "no epochs : 110 and loss : 44.180667877197266\n",
      "no epochs : 120 and loss : 43.79060745239258\n",
      "no epochs : 130 and loss : 43.48969650268555\n",
      "no epochs : 140 and loss : 43.2542724609375\n",
      "no epochs : 150 and loss : 43.0445671081543\n",
      "no epochs : 160 and loss : 42.845829010009766\n",
      "no epochs : 170 and loss : 42.65570068359375\n",
      "no epochs : 180 and loss : 42.479610443115234\n",
      "no epochs : 190 and loss : 42.31001663208008\n",
      "no epochs : 200 and loss : 42.14847183227539\n",
      "no epochs : 210 and loss : 41.986671447753906\n",
      "no epochs : 220 and loss : 41.817481994628906\n",
      "no epochs : 230 and loss : 41.64752197265625\n",
      "no epochs : 240 and loss : 41.47410583496094\n",
      "no epochs : 250 and loss : 41.296756744384766\n",
      "no epochs : 260 and loss : 41.115966796875\n",
      "no epochs : 270 and loss : 40.931461334228516\n",
      "no epochs : 280 and loss : 40.74502944946289\n",
      "no epochs : 290 and loss : 40.5545539855957\n",
      "no epochs : 300 and loss : 40.36134338378906\n",
      "no epochs : 310 and loss : 40.1650505065918\n",
      "no epochs : 320 and loss : 39.96505355834961\n",
      "no epochs : 330 and loss : 39.761714935302734\n",
      "no epochs : 340 and loss : 39.55644989013672\n",
      "no epochs : 350 and loss : 39.35188674926758\n",
      "no epochs : 360 and loss : 39.150630950927734\n",
      "no epochs : 370 and loss : 38.95317077636719\n",
      "no epochs : 380 and loss : 38.751434326171875\n",
      "no epochs : 390 and loss : 38.549373626708984\n",
      "no epochs : 400 and loss : 38.33599090576172\n",
      "no epochs : 410 and loss : 38.1217155456543\n",
      "no epochs : 420 and loss : 37.908546447753906\n",
      "no epochs : 430 and loss : 37.696136474609375\n",
      "no epochs : 440 and loss : 37.483970642089844\n",
      "no epochs : 450 and loss : 37.274417877197266\n",
      "no epochs : 460 and loss : 37.06688690185547\n",
      "no epochs : 470 and loss : 36.86137390136719\n",
      "no epochs : 480 and loss : 36.65781784057617\n",
      "no epochs : 490 and loss : 36.45515823364258\n",
      "no epochs : 500 and loss : 36.253719329833984\n",
      "no epochs : 510 and loss : 36.04997253417969\n",
      "no epochs : 520 and loss : 35.83919906616211\n",
      "no epochs : 530 and loss : 35.62229919433594\n",
      "no epochs : 540 and loss : 35.421260833740234\n",
      "no epochs : 550 and loss : 35.220985412597656\n",
      "no epochs : 560 and loss : 35.02008819580078\n",
      "no epochs : 570 and loss : 34.81528854370117\n",
      "no epochs : 580 and loss : 34.594356536865234\n",
      "no epochs : 590 and loss : 34.34846115112305\n",
      "no epochs : 600 and loss : 34.08829116821289\n",
      "no epochs : 610 and loss : 33.747581481933594\n",
      "no epochs : 620 and loss : 33.42881393432617\n",
      "no epochs : 630 and loss : 33.1190299987793\n",
      "no epochs : 640 and loss : 32.82272720336914\n",
      "no epochs : 650 and loss : 32.51194763183594\n",
      "no epochs : 660 and loss : 32.19999313354492\n",
      "no epochs : 670 and loss : 31.65835952758789\n",
      "no epochs : 680 and loss : 31.13694953918457\n",
      "no epochs : 690 and loss : 30.728424072265625\n",
      "no epochs : 700 and loss : 30.350187301635742\n",
      "no epochs : 710 and loss : 29.97092056274414\n",
      "no epochs : 720 and loss : 29.600025177001953\n",
      "no epochs : 730 and loss : 29.223304748535156\n",
      "no epochs : 740 and loss : 28.843639373779297\n",
      "no epochs : 750 and loss : 28.3527889251709\n",
      "no epochs : 760 and loss : 27.693077087402344\n",
      "no epochs : 770 and loss : 27.18418312072754\n",
      "no epochs : 780 and loss : 26.736461639404297\n",
      "no epochs : 790 and loss : 26.326784133911133\n",
      "no epochs : 800 and loss : 26.002601623535156\n",
      "no epochs : 810 and loss : 25.617490768432617\n",
      "no epochs : 820 and loss : 25.264379501342773\n",
      "no epochs : 830 and loss : 24.943532943725586\n",
      "no epochs : 840 and loss : 24.730823516845703\n",
      "no epochs : 850 and loss : 24.39510726928711\n",
      "no epochs : 860 and loss : 24.06861114501953\n",
      "no epochs : 870 and loss : 23.82172393798828\n",
      "no epochs : 880 and loss : 23.677459716796875\n",
      "no epochs : 890 and loss : 23.369647979736328\n",
      "no epochs : 900 and loss : 23.156030654907227\n",
      "no epochs : 910 and loss : 22.98353385925293\n",
      "no epochs : 920 and loss : 22.784225463867188\n",
      "no epochs : 930 and loss : 22.622148513793945\n",
      "no epochs : 940 and loss : 22.44153594970703\n",
      "no epochs : 950 and loss : 22.27280044555664\n",
      "no epochs : 960 and loss : 22.17265510559082\n",
      "no epochs : 970 and loss : 22.000247955322266\n",
      "no epochs : 980 and loss : 21.86143684387207\n",
      "no epochs : 990 and loss : 21.74344825744629\n",
      "no epochs : 1000 and loss : 21.64012908935547\n",
      "no epochs : 1010 and loss : 21.556177139282227\n",
      "no epochs : 1020 and loss : 21.503448486328125\n",
      "no epochs : 1030 and loss : 21.384153366088867\n",
      "no epochs : 1040 and loss : 21.48671531677246\n",
      "no epochs : 1050 and loss : 21.310548782348633\n",
      "no epochs : 1060 and loss : 21.21183204650879\n",
      "no epochs : 1070 and loss : 21.167285919189453\n",
      "no epochs : 1080 and loss : 21.24986457824707\n",
      "no epochs : 1090 and loss : 21.09307098388672\n",
      "no epochs : 1100 and loss : 21.01869773864746\n",
      "no epochs : 1110 and loss : 20.96390724182129\n",
      "no epochs : 1120 and loss : 21.00238800048828\n",
      "no epochs : 1130 and loss : 20.948266983032227\n",
      "no epochs : 1140 and loss : 20.95751953125\n",
      "no epochs : 1150 and loss : 20.97683334350586\n",
      "no epochs : 1160 and loss : 20.985776901245117\n",
      "no epochs : 1170 and loss : 20.943740844726562\n",
      "no epochs : 1180 and loss : 20.88159942626953\n",
      "no epochs : 1190 and loss : 20.9006404876709\n",
      "no epochs : 1200 and loss : 20.886823654174805\n",
      "no epochs : 1210 and loss : 20.791248321533203\n",
      "no epochs : 1220 and loss : 20.848203659057617\n",
      "no epochs : 1230 and loss : 20.88236427307129\n",
      "no epochs : 1240 and loss : 20.855358123779297\n",
      "no epochs : 1250 and loss : 20.697839736938477\n",
      "no epochs : 1260 and loss : 20.911375045776367\n",
      "no epochs : 1270 and loss : 20.68057632446289\n",
      "no epochs : 1280 and loss : 20.715347290039062\n",
      "no epochs : 1290 and loss : 20.677583694458008\n",
      "no epochs : 1300 and loss : 20.933107376098633\n",
      "no epochs : 1310 and loss : 20.645437240600586\n",
      "no epochs : 1320 and loss : 20.785274505615234\n",
      "no epochs : 1330 and loss : 20.677927017211914\n",
      "no epochs : 1340 and loss : 20.63809585571289\n",
      "no epochs : 1350 and loss : 20.626340866088867\n",
      "no epochs : 1360 and loss : 20.63235092163086\n",
      "no epochs : 1370 and loss : 20.61827850341797\n",
      "no epochs : 1380 and loss : 20.929244995117188\n",
      "no epochs : 1390 and loss : 21.346221923828125\n",
      "no epochs : 1400 and loss : 21.205902099609375\n",
      "no epochs : 1410 and loss : 20.56545639038086\n",
      "no epochs : 1420 and loss : 20.513769149780273\n",
      "no epochs : 1430 and loss : 20.521480560302734\n",
      "no epochs : 1440 and loss : 20.466514587402344\n",
      "no epochs : 1450 and loss : 20.35914421081543\n",
      "no epochs : 1460 and loss : 20.33159637451172\n",
      "no epochs : 1470 and loss : 20.30257797241211\n",
      "no epochs : 1480 and loss : 20.286718368530273\n",
      "no epochs : 1490 and loss : 20.27713966369629\n",
      "no epochs : 1500 and loss : 20.269365310668945\n"
     ]
    }
   ],
   "source": [
    "net = Net(n_feature=3, n_hidden1=20,n_hidden2=10, n_output=1)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.02) # change the learning rate and get high accuracy\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "\n",
    "for t in range(1500):\n",
    "    t = t+1\n",
    "    prediction = net(x)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        print(f\"no epochs : {t} and loss : {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "638df781",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array([pd.DataFrame(list(zip(prediction1,prediction2,prediction3)),columns=[\"prediction1\",\"prediction2\",\"prediction3\"])], dtype=np.float32)\n",
    "x_test = torch.tensor(x_test)\n",
    "prediction = net(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6d2013e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "828a420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.numpy()\n",
    "# first 88.71,88.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5e892907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = model1.predict(testing_data_frame)\n",
    "a = pd.DataFrame(prediction.reshape(-1,1),columns=[\"Match Percentage\"])\n",
    "final = pd.concat([test_files,a],axis=1)\n",
    "final = final.to_csv(\"submission.csv\",index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
